% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/shannon_entropy.R
\name{shannon_entropy}
\alias{shannon_entropy}
\title{Calculate Shannon Entropy}
\usage{
shannon_entropy(probs)
}
\arguments{
\item{probs}{numeric vector.}
}
\value{
numeric
}
\description{
In information theory, the entropy of a random variable is
the average level of "information", "surprise", or "uncertainty"
inherent in the variable's possible outcomes. The concept of
information entropy was introduced by Claude Shannon
in his 1948 paper "A Mathematical Theory of Communication
}
\examples{
p <- c(6 / 26, 20 / 26)
shannon_entropy(p)
}
